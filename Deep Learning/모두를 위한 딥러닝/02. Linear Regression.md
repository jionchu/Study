# Hypothesis & Cost of Linear Regression
### (Linear) Hypothesis
- H(x) = W(x)+b
- 실제 데이터에 맞는 선을 찾는 것

### Cost(Lost) function
- 가설과 실제 데이터의 차이 (가장 좋은 가설 찾기 위한 값)

#### minimize cost(W,b)를 구하는 것이 학습의 목표

# How to minimize cost
cost function을 어떻게 minimize하여 최종적으로 linear regression 학습을 완성하는가

### Simplified hypothesis
계산의 편의를 위해 식을 간략히 바꿈
- H(x) = Wx

ex) x =1, Y=1 / x=2, Y=2 / x=3, Y=3  
W=1, cost(W)=0  
W=0, cost(W)=4.67  
W=2, cost(W)=4.67  
=> W 값의 변화에 따라 cosst 값이 어떻게 변화하는지 그래프 그려보기  
![cost graph](https://github.com/jionchu/Study/blob/master/Deep%20Learning/%EB%AA%A8%EB%91%90%EB%A5%BC%20%EC%9C%84%ED%95%9C%20%EB%94%A5%EB%9F%AC%EB%8B%9D/images/cost-graph.png)  
=> cost 값이 가장 작은 W 알아내기

### Gradient descent algorithm (경사를 따라 내려가는 알고리즘)
- Minimize cost function
- Gradient descent is used many minimization problems
- For a gien cost function, cost(W,b), it will find W, b to minimize cost
- It can be applied to more general function: cost (W1, W2, ...)
- 많은 값들이 있는 cost function도 minimize할 수 있는 알고리즘

### How it works?
- Start with initial guesses
- Start at 0,0 (or any other value)
- Keeping changing W and b a little bit to try and reduce cost(W,b)
- 항상 최저점에 도달할 수 있다는 것이 이 알고리즘의 장점
- 경사도는 미분을 이용하여 구함
-  => 여러 번 실행시키면서 W 값이 변하면 최종적으로 그 값이 cost를 minimize하는 값이 자동으로 구해짐
- Each time you change the parameters, you select the gradient which reduces cost(W,b) the most possible
- Repeat
- Do so until you converge to a local minimum
- Has an interesting property
- Where you start can determine which minimum you end up

### Convex function (아래로 볼록)
![non-convex function](https://github.com/jionchu/Study/blob/master/Deep%20Learning/%EB%AA%A8%EB%91%90%EB%A5%BC%20%EC%9C%84%ED%95%9C%20%EB%94%A5%EB%9F%AC%EB%8B%9D/images/non-convex-function.png)  
- cost function이 위와 같은 경우(convex function이 아닌 경우) 시작점에 따라 결과값이 다르게 나옴 => 알고리즘이 제대로 동작하지 않음  
![convex function](https://github.com/jionchu/Study/blob/master/Deep%20Learning/%EB%AA%A8%EB%91%90%EB%A5%BC%20%EC%9C%84%ED%95%9C%20%EB%94%A5%EB%9F%AC%EB%8B%9D/images/convex-function.png)  
- convex function의 경우 Gradient decent algorithm이 항상 답을 찾음
- Linear regression을 적용하기 전에 cost function이 convex function 모양인지 확인해야 함
