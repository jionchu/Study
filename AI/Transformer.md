# Transformer

## 1. 트랜스포머(Transformer)
- 기존의 seq2seq의 구조인 인코더-디코더를 따르면서도 어텐션만으로 구현한 모델
- RNN, LSTM 없이 time 시퀀스 역할을 하는 모델
- RNN, LSTM 셀을 일체 사용하지 않았으나, 자체만으로 time 시퀀스 역할을 해줄 수 있음
- seq2seq 구조에서는 인코더와 디코더에서 각각 하나의 RNN이 t개의 시점을 가지는 구조였다면 이번에는 인코더와 디코더라는 단위가 N개로 구성되는 구조이다.

- 시작 심볼 <sos>를 입력으로 받아 종료 심볼 <eos>가 나올 때까지 연산을 진행

## 1. 포지셔널 인코딩(Positional Encoding)
- 단어 입력을 순차적으로 받는 방식이 아니므로 단어의 위치 정보를 다른 방식으로 알려줄 필요가 있음
- 단어의 위치 정보를 얻기 위해서 각 단어의 임베딩 벡터에 위치 정보들을 더하여 모델의 입력으로 사용하는데, 이를 포지셔널 인코딩이라고 함

## 2. 인코더(Encoder)
하나의 인코더는 크게 2개의 서브층으로 나누어짐
- self-attention
- feed forward neural network

## 3. 인코더의 멀티-헤드 어텐션
### 3.1 Self-Attention의 의미와 이점
어텐션을 자기 자신에게 수행한다는 의미
- 입력 문장 내의 단어들끼리 유사도를 구함
