# Attention Machanism

- RNN에 기반한 seq2seq 모델의 문제점
  - 하나의 고정된 크기의 벡터에 모든 정보를 압축하려고 하니까 정보 손실이 발생함
  - RNN의 고질적인 문제인 기울기 소실 (Vanishing Gradient) 문제가 존재

→ 기계 번역 분야에서 입력 문장이 길면 번역 품질이 떨어지는 현상으로 나타남  
→ 이를 위한 대안으로 입력 시퀀스가 길어지면 출력 시퀀스의 정확도가 떨어지는 것을 보정해주기 위해 **attention**이 등장

## 1. Attention의 아이디어
- 디코더에서 출력 단어를 예측하는 매 시점(time step)마다, 인코더에서의 전체 입력 문장을 다시 한 번 참고
- 단, 전체 입력 문장을 전부 다 동일한 비율로 참고하는 것이 아니라, 해당 시점에서 예측해야할 단어와 연관이 있는 입력 단어 부분을 좀 더 집중(attention)해서 보게 됨

## 2. Attention Function
어텐션을 함수로 표현하면 다음과 같다.
- Attention(Q, K, V) = Attention Value

주어진 쿼리(Query)에 대해서 모든 키(Key)와의 유사도를 각각 구한다. 그리고 구해낸 유사도를 키와 매핑되어있는 각각의 값(Value)에 반영해준다. 그리고 유사도가 반영된 값(Value)을 모두 더해서 리턴한다. 이 값을 어텐션 값(Attention Value)이라고 한다.  

- Q = Query : t 시점의 디코더 셀에서의 은닉 상태
- K = Keys : 모든 시점의 인코더 셀의 은닉 상태들
- V = Values : 모든 시점의 인코더 셀의 은닉 상태들

## 3. Dot-Product Attention
다양한 종류의 어텐션 중 가장 수식적으로 이해하기 쉽게 수식을 적용한 어텐션  

- 인코더의 softmax 함수를 통해 나온 결과값은 input의 각 단어들이 출력 단어를 예측할 때 얼마나 도움이 되는지의 정도를 수치화한 값
- 이를 하나의 정보로 담아서 디코더로 전송 → 디코더는 출력 단어를 더 정확하게 예측할 확률이 높아짐

### 3.1 어텐션 스코어(Attention Score) 구하기
인코더의 시점을 1, 2, ... N이라고 했을 때 인코더의 은닉 상태(hidden state)를 각각 h1, h2, ... hN이라고 하고, 디코더의 현재 시점(time step) t에서의 디코더의 은닉 상태를 st라고 하자. 여기에서 인코더의 은닉 상태와 디코더의 은닉 상태의 차원은 같다고 가정

시점 t에서 출력 단어를 예측하기 위해 디코더의 셀은 두 개의 입력값을 필요로 함
- 이전 시점인 t-1의 은닉 상태
- 이전 시점 t-1에 나온 출력 단어

어텐션 메커니즘에서는 출력 단어 예측에 또 다른 값을 필요로 함
- 어텐션 값(Attention Value)

어텐션 스코어란
- 현재 디코더의 시점 t에서 단어를 예측하기 위해, 인코더의 모든 은닉 상태 각각이 디코더의 현 시점의 은닉 상태 st와 얼마나 유사한지 판단하는 스코어값

dot-product attention에서는 이 스코어 값을 구하기 위해 st를 전치(transpose)하고 각 은닉 상태와 내적(dot product)을 수행함 → 모든 어텐션 스코어 값은 스칼라

score(st, hi) = stT hi

st와 인코더의 모든 은닉 상태의 어텐션 스코어의 모음값을 et라고 정의

### 3.2 softmax 함수를 통해 어텐션 분포(Attention Distribution) 구하기
et에 softmax 함수를 적용하여 모든 값을 합하면 1이 되는 확률 분포를 얻어냄 → 이를 어텐션 분포(Attention Distribution)라고 함

디코더의 시점 t에서 어텐션 분포를 alpha_t라고 할 때  
alpha_t = softmax(et)

### 3.3 어텐션 값(Attention Value) 구하기
각 인코더의 은닉 상태(hi)와 어텐션 가중치값(alpha_t)들을 곱하고 최종적으로 모두 더한다.

이러한 어텐션 값 at은 종종 인코더의 문맥을 포함하고 있다고하여 **컨텍스트 벡터(context vector)**라고도 불림

### 3.4 어텐션 값과 디코더의 t 시점의 은닉 상태 연결 (Concatenate)
어텐션 값 at와 디코더의 은닉 상태 st를 결합하여 하나의 벡터로 만든다. 이를 v_t라고 정의하고 이 v_t를 y_hat 예측의 연산의 입력으로 사용
